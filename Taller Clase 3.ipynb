{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Taller Clase 3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"olrHPnG2pH4l","colab_type":"text"},"source":["# Artificial Intelligence Workshop Day 3\n","\n","By Juan-Pablo Silva (https://github.com/juanpablos) jpsilva@dcc.uchile.cl, Alexandre Bergel (abergel@dcc.uchile.cl) and Alejandro Infante (ainfante@dcc.uchile.cl)."]},{"cell_type":"markdown","metadata":{"id":"GLfl1eRMpRbG","colab_type":"text"},"source":["## Install packages and set environments\n","\n","We will be using the [Open AI Gym learning environment](https://github.com/openai/gym) as our base to executing games, and the [NEAT Python](https://neat-python.readthedocs.io/en/latest/) package for a python implementation of the NEAT (NeuroEvolution of Augmenting Topologies) algorithm.\n","\n","Because Google Colab does not have rendering hardware, not a physical screen, we will have to workaround this by creating a virtual screen and rendering a video. The original code for this was done by William Xu, and described in the following blog post: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/."]},{"cell_type":"markdown","metadata":{"id":"lEj3jtUQq34Z","colab_type":"text"},"source":["Install system packages."]},{"cell_type":"code","metadata":{"id":"usWOdQW0jav5","colab_type":"code","colab":{}},"source":["!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!apt-get install pkg-config lua5.1 build-essential git > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"woQ4E31KqsuG","colab_type":"text"},"source":["Instal python packages."]},{"cell_type":"code","metadata":{"id":"UKifn3XDjgzY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"9655a32a-d8b3-439b-a518-a43ffcf7e33a","executionInfo":{"status":"ok","timestamp":1575412365755,"user_tz":180,"elapsed":42288,"user":{"displayName":"Alexandre Bergel","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBsrIuFGQkxCqQcw8Zd2mZKPSIMyyAXhU0mW0HJIA=s64","userId":"02563535896661050156"}}},"source":["!pip install gym==0.13.1 pyvirtualdisplay > /dev/null 2>&1 \n","!pip install tqdm gym-retro > /dev/null 2>&1\n","!pip install -U git+git://github.com/frenchie4111/dumbrain.git > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install box2d-py > /dev/null 2>&1\n","!pip install neat-python cloudpickle opencv-python > /dev/null 2>&1\n","!pip install gym[all] > /dev/null 2>&1"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (42.0.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9ErGG-VaCMVf","colab_type":"code","colab":{}},"source":["# The gym package to import our games\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","\n","# The python neat package\n","import neat\n","\n","# Plotting\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Utilities and rendering\n","import numpy as np\n","import random\n","import math\n","import time\n","import pickle\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_EsI0uivES5","colab_type":"text"},"source":["## The NEAT package\n","\n","The Python NEAT package provides a general framework for using genetic algorithms applied to neural networks. To use it we need to import the `neat` package and define a `population`, `fitness function` and a `configuration file`. The first 2 are defined in python. The config file is a separate file that contains several parameters and values to fully customize the network and how it will be mutating over time. We will not be discussing this in the session and are providing you with all the configuration files you need. In case you want to know more details, please read the [documentation](https://neat-python.readthedocs.io/en/latest/config_file.html) on this topic.\n","Feel free to modify the configuration files we provide to see what can each option change in the process."]},{"cell_type":"markdown","metadata":{"id":"W4dUZORB56nL","colab_type":"text"},"source":["Download the configuration files."]},{"cell_type":"code","metadata":{"id":"rpm-jzqE6CSq","colab_type":"code","colab":{}},"source":["!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/config-bipedal-walker > /dev/null 2>&1 \n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/config-cart-pole > /dev/null 2>&1 \n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/config-lunar > /dev/null 2>&1 \n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/config-lunar-rec > /dev/null 2>&1 \n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/config-mountain > /dev/null 2>&1 \n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/config-mountain-rec > /dev/null 2>&1 \n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/config-xor > /dev/null 2>&1 \n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/config-and > /dev/null 2>&1 "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vq7FgHalssYX","colab_type":"text"},"source":["### Learning with NEAT\n"," When we use the NEAT algorithm, we have to understand that we are using an AI, to create another AI. We are not training on a dataset, nor teaching the network how it should perform. Based on evaluations and feedback (fitness), the NEAT algorithm is capable of optimizing the parameters and topology (how many layers, how many neurons, and what neurons and connected to which) of a population of neural networks to solve a particular problem.\n"," \n"," Let's take a look at how we would implement the AND logic gate that we used in Day 1."]},{"cell_type":"markdown","metadata":{"id":"VNoPEOTU0qCl","colab_type":"text"},"source":["First let's define a function to let us visualize how the fitness evolved, and some cool statistics about the generation process."]},{"cell_type":"code","metadata":{"id":"VTXvYvQaeBMo","colab_type":"code","colab":{}},"source":["def plot_neat_info(statistics, ylog=False, view=False, filename='neat.png'):\n","    # general stats\n","    generation = range(len(statistics.most_fit_genomes))\n","    best_fitness = [c.fitness for c in statistics.most_fit_genomes]\n","    avg_fitness = np.array(statistics.get_fitness_mean())\n","    stdev_fitness = np.array(statistics.get_fitness_stdev())\n","    \n","    # species\n","    species_sizes = statistics.get_species_sizes()\n","    num_generations = len(species_sizes)\n","    curves = np.array(species_sizes).T\n","    \n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n","    \n","    ax1.plot(generation, avg_fitness, 'b-', label=\"average\")\n","    ax1.plot(generation, avg_fitness - stdev_fitness, 'g-.', label=\"-std\")\n","    ax1.plot(generation, avg_fitness + stdev_fitness, 'g-.', label=\"+std\")\n","    ax1.plot(generation, best_fitness, 'r-', label=\"best\")\n","    ax1.set_title(\"Population's average and best fitness\")\n","    ax1.set_xlabel(\"Generations\")\n","    ax1.set_ylabel(\"Fitness\")\n","    ax1.grid()\n","    ax1.legend(loc=\"best\")\n","    \n","    ax2.stackplot(range(num_generations), *curves)\n","    ax2.set_title(\"Speciation\")\n","    ax2.set_ylabel(\"Size per Species\")\n","    ax2.set_xlabel(\"Generations\")\n","    \n","    if ylog:\n","        ax1.gca().set_yscale('symlog')\n","        \n","    fig.suptitle('NEAT traning stats')\n","\n","    plt.savefig(filename)\n","    if view:\n","        plt.show()\n","\n","    plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6OEKyxKyq-hV","colab_type":"text"},"source":["Here we define the AND logic gate inputs and the fitness function."]},{"cell_type":"code","metadata":{"id":"aTx2DKXQq-MD","colab_type":"code","colab":{}},"source":["# Let's define a function for it to be more flexible\n","def run_and():\n","    # the inputs and outputs of the logic gate\n","    and_inputs = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n","    and_outputs = [[0.0], [0.0], [0.0], [1.0]]\n","\n","    # This would be our fitness function\n","    # what we are evaluating here is how good our generated networks\n","    # are representing the output values.\n","    # In this case a perfect score would be 0, as in 0 errors.\n","    # For each mistake the network makes, we will be subtracting\n","    # the square error of the output (just like on a traditional MSE loss).\n","    # The worst score possible would be -4.\n","    def eval_genomes(genomes, config):\n","        # The neat package gives us a list of all individuals\n","        # so we have to test them all\n","        for genome_id, genome in genomes:\n","            # Each individual represents a network structure and weights\n","            # so we just need to create a neural network out of them\n","            net = neat.nn.FeedForwardNetwork.create(genome, config)\n","            # Let's make each network start with a fitness of 0.\n","            # This is an arbitrary decision. We could have used a starting\n","            # fitness of 4.0, and make the best score 4, and the min 0.\n","            genome.fitness = 0.\n","            # Testing the individual performance\n","            for x, y in zip(and_inputs, and_outputs):\n","                # Get an output from the network\n","                output = net.activate(x)\n","                # MSE and setting the fitness\n","                genome.fitness -= (output[0] - y[0]) ** 2\n","\n","    def run(config_file):\n","        # Load the configuration files.\n","        # Generally you won't need to touch this\n","        config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n","                             neat.DefaultSpeciesSet, neat.DefaultStagnation,\n","                             config_file)\n","\n","        # Create the population.\n","        pop = neat.Population(config)\n","\n","        # Make NEAT to print some nice stat values as it trains\n","        stats = neat.StatisticsReporter()\n","        pop.add_reporter(stats)\n","        # comment this line if the output is too annoying\n","        # pop.add_reporter(neat.StdOutReporter(True))\n","        \n","\n","        # Run for up to 100 generations.\n","        # If the solution is found before that, stop.\n","        winner = pop.run(eval_genomes, 100)\n","\n","        # Print the output values of the best individual\n","        # and plot some training information\n","        print('Output:')\n","        winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n","        for xi, xo in zip(and_inputs, and_outputs):\n","            output = winner_net.activate(xi)\n","            print(\"\\tinput {!r}, expected output {!r}, got {!r}\".format(xi, xo, output))\n","\n","        # plot training stas\n","        plot_neat_info(stats, ylog=False, view=True, filename='stats_and.png')\n","        \n","    # the name of the configuration file\n","    run('config-and')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5oNMLe4ZrwTt","colab_type":"code","colab":{}},"source":["# Run the AND logic gate example\n","run_and()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rP05tCiHr5sD","colab_type":"text"},"source":["### Exercise: Implement XOR logic gate\n","Based on the AND example above, implement the XOR logic gate and see how the generated networks perform."]},{"cell_type":"code","metadata":{"id":"PXcgFNzheT3v","colab_type":"code","colab":{}},"source":["# Let's define a function for it to be more flexible\n","def run_xor():\n","    # Something must go here...\n","    # Remember you need to provide examples\n","    #########################\n","    \n","    def eval_genomes(genomes, config):\n","        # You have to complete this function\n","        #########################\n","        pass\n","\n","    def run(config_file):\n","        # Load the configuration files.\n","        # Generally you won't need to touch this\n","        config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n","                             neat.DefaultSpeciesSet, neat.DefaultStagnation,\n","                             config_file)\n","\n","        # Create the population.\n","        pop = neat.Population(config)\n","\n","        # Make NEAT to print some nice stat values as it trains\n","        stats = neat.StatisticsReporter()\n","        pop.add_reporter(neat.StdOutReporter(True))\n","        pop.add_reporter(stats)\n","\n","        # Run for up to 100 generations.\n","        # If the solution is found before that, stop.\n","        winner = pop.run(eval_genomes, 100)\n","\n","        # Print the output values of the best individual\n","        # and plot some training information\n","        #########################\n","        ...\n","\n","            \n","        print(stats)\n","        # plot training stas\n","        plot_neat_info(stats, ylog=False, view=True, filename='stats_xor.png')\n","        \n","    # the name of the configuration file\n","    run('config-xor')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4d0ysrurfXll","colab_type":"code","colab":{}},"source":["# Run the XOR logic gate exercise\n","run_xor()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sk8fCid5xWEB","colab_type":"text"},"source":["## The Gym Environment\n","\n","Generally used to compare and train Reinforcement Learning algorithms, the Open AI Gym environment can still be used to run on a NEAT policy.\n","\n","Then we need to implement the workaround for colab not having a screen..."]},{"cell_type":"code","metadata":{"id":"raus9-4fCWNq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"c547ea48-4326-4f82-db31-4f2d6fd0fb93","executionInfo":{"status":"ok","timestamp":1575412104622,"user_tz":180,"elapsed":69116,"user":{"displayName":"Alexandre Bergel","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBsrIuFGQkxCqQcw8Zd2mZKPSIMyyAXhU0mW0HJIA=s64","userId":"02563535896661050156"}}},"source":["######################\n","# ---- don't touch this -----\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(300, 300))\n","display.start()\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","    mp4list = glob.glob('video/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                    loop controls style=\"height: 400px;\">\n","                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","                 </video>'''.format(encoded.decode('ascii'))))\n","    else: \n","        print(\"Could not find video\")\n","\n","\n","def wrap_env(env):\n","    env = Monitor(env, './video', force=True)\n","    return env\n","# ---- don't touch this -----\n","######################"],"execution_count":3,"outputs":[{"output_type":"stream","text":["xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"8uyXIdNsW2Si","colab_type":"text"},"source":["### Cart Pole example\n","\n","The Cart Pole environment consists on balancing a pole over a cart only by pushing the cart to the right or left. Intuitively the pole will start falling to one side and the cart should compensate that falling by moving to that site. This first example shows a simple heuristic where we push the cart to the left if the pole is moving to the left and push the cart to the right otherwise."]},{"cell_type":"code","metadata":{"id":"DichKj0MW3x5","colab_type":"code","colab":{}},"source":["# Example of the Cart-Pole environment. Balancing a pole of length 1.0\n","# Observation: \n","#        Type: Box(4)\n","#        Num\tObservation                 Min         Max\n","#        0\tCart Position             -4.8            4.8\n","#        1\tCart Velocity             -Inf            Inf\n","#        2\tPole Angle                 -24 deg        24 deg\n","#        3\tPole Velocity At Tip      -Inf            Inf\n","#        \n","#    Actions:\n","#        Type: Discrete(2)\n","#        Num\tAction\n","#        0\tPush cart to the left\n","#        1\tPush cart to the right\n","\n","env = wrap_env(gym.make(\"CartPole-v1\"))\n","env.reset()\n","default_action = 1 #Default action\n","observation = env.state\n","for t in range(1000):\n","    env.render()\n","    if observation[3] < -0.3: # Evaluate if pole is moving to the left\n","      #action = 0 #Push the cart to the left\n","      default_action = 0\n","    if observation[3] > 0.3: # Evaluate if pole is moving to the right\n","      action = 1 #Push the cart to the right\n","      default_action = 1\n","    action = default_action \n","      \n","    observation, reward, done, info = env.step(action) #Execute action and update observed state\n","    if done:\n","        break\n","            \n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RdrouyknW-AF","colab_type":"text"},"source":["### Exercise: Implement simple heuristic for MountainCart\n","\n","Another environment in Gym is the Mountain Car environment, which is basically a car trying to climb a hill. The way to do it is by gaining speed by going the opposite direction first. Here we have 2 things to look at (observations): the velocity and the position of the car. We also have 3 possible actions to perform: push left, no push, push right.\n","\n","The following cells shows a random agent trying, and failing, to solve the task."]},{"cell_type":"code","metadata":{"id":"fPbxIWEKW-3I","colab_type":"code","colab":{}},"source":["# Example of the MountainCar environment taking random actions\n","# Observation: \n","#        Type: Box(2)\n","#        Num\tObservation                 Min         Max\n","#        0\tCart Position             -1.2            0.6\n","#        1\tCart Velocity             -0.07           0.07\n","#        \n","#    Actions:\n","#        Type: Discrete(3)\n","#        Num\tAction\n","#        0\tPush cart to the left\n","#        1\tDo nothing\n","#        2\tPush cart to the right\n","env = wrap_env(gym.make(\"MountainCar-v0\"))\n","env.reset()\n","for t in range(1000):\n","    env.render()\n","    action = env.action_space.sample()\n","    observation, reward, done, info = env.step(action)\n","    if done:\n","        break\n","            \n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nzOehAxqXCf3","colab_type":"text"},"source":["**Exercise: Implement simple heuristic for MountainCart**\n","\n","In this exercise you need to implement a simple heuristic based on the current speed of the cart. The heuristic consists on pushing the cart in the direction of the movement of it to increase its momentum. Implement it in the code below where it is pointed in with the comments and then run the code block to see how your code performs."]},{"cell_type":"code","metadata":{"id":"P5OUuQAmXFlz","colab_type":"code","colab":{}},"source":["# Excercise of the MountainCar environment\n","# Observation: \n","#        Type: Box(2)\n","#        Num\tObservation                 Min         Max\n","#        0\tCart Position             -1.2            0.6\n","#        1\tCart Velocity             -0.07           0.07\n","#        \n","#    Actions:\n","#        Type: Discrete(3)\n","#        Num\tAction\n","#        0\tPush cart to the left\n","#        1\tDo nothing\n","#        2\tPush cart to the right\n","env = wrap_env(gym.make(\"MountainCar-v0\"))\n","env.reset()\n","observation = env.state\n","for t in range(1000):\n","    env.render()\n","    ##### Write your code here #####\n","    action = 0\n","    ### Answer\n","    ...\n","    ##### End of your code #####\n","    observation, reward, done, info = env.step(action)\n","    # observation -> (position, velocity)\n","    if done:\n","        break\n","            \n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zdXIHDhPXHvt","colab_type":"text"},"source":["### Bipedal Walker environment\n","The Bipedal Walker environment is about teaching a bipedal -something- how to walk through a rough terrain. In this case we have 24 possible variables we can observate, from the hip, knee and leg angles, to the position and velocity. Likewise, we have 4 possible actions, apply torque to the hips and knees. \n","\n","As you can imagine, this problem is more difficult than the previous one and you can see how it behaves when it takes random actions."]},{"cell_type":"code","metadata":{"id":"kHUyY3FBXKoc","colab_type":"code","colab":{}},"source":["# Example of the BipedalWalker environment taking random actions\n","env = wrap_env(gym.make(\"BipedalWalker-v2\"))\n","env.reset()\n","for t in range(1000):\n","    env.render()\n","    action = env.action_space.sample()\n","    observation, reward, done, info = env.step(action)\n","    if done:\n","        break\n","            \n","env.close()\n","show_video()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9lVKvmvB0KwP","colab_type":"text"},"source":["## NEAT and Gym\n","Here comes the fun part. Let's use our NEAT algorithm to try and generate a neural network capable of solving the task of some games. These training processes take time, so be patient and wait for the fun results!"]},{"cell_type":"markdown","metadata":{"id":"-u8rFJKs04W5","colab_type":"text"},"source":["### General Gym NEAT solver\n","The following class presents a general approach to solving any problem from the Gym environment. The class needs you to supply a game name (check some in the [Gym documentation](https://gym.openai.com/envs/)), and a configuration file for that environment. The configuration files does not differ much from game to game, but you must change the network input and output number to fit the corresponding environment.\n","In case the game uses a multi action setup, for example pressing multiple buttons at once, you must set the `multi_output` flag to `True`."]},{"cell_type":"code","metadata":{"id":"Bw0antO2NXzg","colab_type":"code","colab":{}},"source":["class GymNEAT:\n","    def __init__(self, game_name, config_file, verbose=True, winner_out=\"winner.pkl\",\n","                load_checkpoint=None, use_recurrent=False, multi_output=False):\n","        self.game_name = game_name\n","        self.env = None\n","        self.config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n","                                  neat.DefaultSpeciesSet, neat.DefaultStagnation,\n","                                  config_file)\n","        if load_checkpoint:\n","            self.pop = neat.Checkpointer.restore_checkpoint(load_checkpoint)\n","        else:\n","            self.pop = neat.Population(self.config)\n","            \n","        self.stats = neat.StatisticsReporter()\n","        self.pop.add_reporter(self.stats)\n","        self.pop.add_reporter(neat.StdOutReporter(verbose))\n","        # Checkpoint every 25 generations or 900 seconds.\n","        self.pop.add_reporter(neat.Checkpointer(25, 900, filename_prefix=self.game_name+\"-\"))  \n","        \n","        self.winner_file = winner_out\n","        self.use_recurrent = use_recurrent\n","        self.box_output = multi_output\n","    \n","    def eval_fitness(self, genomes, config):\n","        for genome_id, genome in genomes:\n","            genome.fitness = self.eval_single_genome(genome, config)\n","\n","    def eval_single_genome(self, genome, config):\n","        if self.use_recurrent:\n","            net = neat.nn.recurrent.RecurrentNetwork.create(genome, config)\n","        else:\n","            net = neat.nn.FeedForwardNetwork.create(genome, config)\n","        total_reward = 0.0\n","\n","        for i in range(self.episodes):\n","            observation = self.env.reset()\n","\n","            action = self.eval_network(net, observation)\n","            done = False\n","            while not done:\n","                observation, reward, done, info = self.env.step(action)\n","                action = self.eval_network(net, observation)\n","                total_reward += reward\n","                if done:\n","                    break\n","                    \n","        return total_reward / self.episodes\n","    \n","    def eval_network(self, net, observation):\n","        action = net.activate(observation)\n","        if not self.box_output:\n","            action = np.argmax(action)\n","        return action\n","    \n","    def plot_stats(self):\n","        plot_neat_info(self.stats, ylog=False, view=True, \n","                       filename='neat-{}.png'.format(self.game_name))\n","                     \n","    def run_test(self):\n","        with open(self.winner_file, 'rb') as f:\n","            best = pickle.load(f)\n","\n","        if self.use_recurrent:\n","            winner_net = neat.nn.recurrent.RecurrentNetwork.create(best, self.config)\n","        else:\n","            winner_net = neat.nn.FeedForwardNetwork.create(best, self.config)\n","\n","        env = wrap_env(gym.make(self.game_name))\n","        \n","        done = False\n","        observation = env.reset()\n","        action = self.eval_network(winner_net, observation)\n","        while not done:\n","            env.render()\n","            observation, reward, done, info = env.step(action)\n","            action = self.eval_network(winner_net, observation)\n","            if done:\n","                break\n","\n","        env.close()\n","        show_video()\n","    \n","    def run(self, iterations, episodes=10):\n","        self.env = gym.make(self.game_name)\n","        self.episodes = episodes\n","        winner = self.pop.run(self.eval_fitness, iterations)\n","        self.env.close()\n","        with open(self.winner_file, 'wb') as win_file:\n","            pickle.dump(winner, win_file)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P85LB3zw2GVF","colab_type":"text"},"source":["To use the class, just create a GymNEAT object with the corresponding parameters. Then call the `run` method to start the environment. The `run` method requires you to pass the maximum number of generations to do and the number of episodes each individual have to be tested on. The number of episodes is a custom in the game learning community because a particular individual could archive the task by luck one time and then never again. Forcing it to do it correctly a number of times decrease the chance if it being just luck."]},{"cell_type":"markdown","metadata":{"id":"JuHgPBVd3FyV","colab_type":"text"},"source":["Run the following cell to see how our NEAT algorithm can create a neural network that can balance a pole on top of a cart."]},{"cell_type":"code","metadata":{"id":"F26x8fJcwav2","colab_type":"code","colab":{}},"source":["# create the GymNEAT object\n","cart_pole = GymNEAT(game_name=\"CartPole-v1\", config_file=\"config-cart-pole\", \n","                    verbose=True, winner_out=\"winner_cart.pkl\")\n","# Run the algorithm\n","cart_pole.run(100, 10)\n","# Create a video out of the best individual!\n","cart_pole.run_test()\n","# See some nerdy statistics about the algorithm process!\n","cart_pole.plot_stats()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2UKddsqH3UF1","colab_type":"text"},"source":["Here we are trying to solve the task of getting on top of a hill."]},{"cell_type":"code","metadata":{"id":"AGjfhbUnxmsq","colab_type":"code","colab":{}},"source":["mount_car = GymNEAT(game_name=\"MountainCar-v0\", config_file=\"config-mountain\", \n","                    verbose=True, winner_out=\"winner_mountain.pkl\")\n","mount_car.run(100, 10)\n","mount_car.run_test()\n","mount_car.plot_stats()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WN7zrQEF48RU","colab_type":"text"},"source":["Let's try with a more sophisticated neural network: a recurrent neural network."]},{"cell_type":"code","metadata":{"id":"Udtt2u2l47WT","colab_type":"code","colab":{}},"source":["mount_car_rec = GymNEAT(game_name=\"MountainCar-v0\", config_file=\"config-mountain-rec\", \n","                        verbose=True, winner_out=\"winner_mountain_rec.pkl\", use_recurrent=True)\n","mount_car_rec.run(100, 10)\n","mount_car_rec.run_test()\n","mount_car_rec.plot_stats()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0Tzjdzy3roo","colab_type":"text"},"source":["Can we generate a neural network that can walk? Are we saying that this AI is teaching itself how to walk?!\n","\n","We provide an almost finished generation process. Run the following cell and see a neural network that can _almost_ walk pretty well. The last generation takes around 5 minutes to finish. We provide some other checkpoints in the training history, you can run them and see how the networks have improved over time."]},{"cell_type":"code","metadata":{"id":"P6yUH79o4Epd","colab_type":"code","colab":{}},"source":["bipedal = GymNEAT(game_name=\"BipedalWalker-v2\", config_file=\"config-bipedal-walker\", \n","                  verbose=True, winner_out=\"winner_bipedal_pre.pkl\", multi_output=True)\n","#bipedal.run(100, 10)\n","bipedal.run_test()\n","#bipedal.plot_stats()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eg2dpU2G34-y","colab_type":"text"},"source":["These levels are really hard. What we try to do is landing safely in the moon."]},{"cell_type":"code","metadata":{"id":"3oA1KnKgJVly","colab_type":"code","colab":{}},"source":["lunar_rec = GymNEAT(game_name=\"LunarLanderContinuous-v2\", config_file=\"config-lunar-rec\", \n","                    verbose=True, winner_out=\"winner_lunar_rec.pkl\", use_recurrent=True, multi_output=True)\n","lunar_rec.run(100, 10)\n","lunar_rec.run_test()\n","lunar_rec.plot_stats()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m_c-qTK28U6z","colab_type":"text"},"source":["## Solutions\n","\n","Some of these examples can take hours to train and obtain good results. Here are some trained examples you can use to see the best performance.\n","\n","Download these files:"]},{"cell_type":"code","metadata":{"id":"f5gwu80xKxRC","colab_type":"code","colab":{}},"source":["!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/BipedalWalker-v2-20\n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/BipedalWalker-v2-57\n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/BipedalWalker-v2-98\n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/LunarLander-v2-99\n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/LunarLander-v2-174\n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/LunarLander-v2-349\n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/winner_bipedal_pre.pkl\n","!wget https://raw.githubusercontent.com/juanpablos/config-files-neat/master/winner_mountain_rec_pre.pkl"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtrzY0xu89Q2","colab_type":"text"},"source":["For the BipedalWalker, we can load the genes for the generations 20, 57 and 98. Here we show the best individuals of those generations."]},{"cell_type":"code","metadata":{"id":"OAUMA2nJ8_CD","colab_type":"code","colab":{}},"source":["bipedal_solutions = []\n","for saved_iteration in [20, 57, 98]:\n","    bipedal = GymNEAT(game_name=\"BipedalWalker-v2\", config_file=\"config-bipedal-walker\", \n","                      verbose=True, winner_out=f\"winner_bipedal_{saved_iteration}.pkl\", \n","                      load_checkpoint=f\"BipedalWalker-v2-{saved_iteration}\", multi_output=True)\n","    bipedal.run(1, 1)\n","    bipedal_solutions.append(bipedal)\n","    \n","for bipedal in bipedal_solutions:\n","    bipedal.run_test()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U9BEPDWo_AaU","colab_type":"text"},"source":["To load the serialized winner model for the Bipedal, run this cell."]},{"cell_type":"code","metadata":{"id":"1G0KDSIc-gsL","colab_type":"code","colab":{}},"source":["bipedal_solution = GymNEAT(game_name=\"BipedalWalker-v2\", config_file=\"config-bipedal-walker\", \n","                  verbose=True, winner_out=\"winner_bipedal_pre.pkl\", multi_output=True)\n","bipedal_solution.run_test()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yQHBvQgJ_KBO","colab_type":"text"},"source":["The LunarLander is a really hard problem to be solved with NEAT, so even after 349 iterations, it still fails miserably."]},{"cell_type":"code","metadata":{"id":"etpI8bmy-GAR","colab_type":"code","colab":{}},"source":["lunar_rec = GymNEAT(game_name=\"LunarLanderContinuous-v2\", config_file=\"config-lunar-rec\", \n","                    load_checkpoint=\"LunarLander-v2-349\",\n","                    verbose=True, winner_out=\"winner_lunar_rec.pkl\", use_recurrent=True, multi_output=True)\n","lunar_rec.run(1, 1)\n","lunar_rec.run_test()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCaALTyJ_YvS","colab_type":"text"},"source":["The MountainCar is not a hard problem, here we load a serialized model that can solve the MountainCar environment."]},{"cell_type":"code","metadata":{"id":"51iVvi0m-zAV","colab_type":"code","colab":{}},"source":["mount_car_rec = GymNEAT(game_name=\"MountainCar-v0\", config_file=\"config-mountain-rec\", \n","                        verbose=True, winner_out=\"winner_mountain_rec_pre.pkl\", use_recurrent=True)\n","mount_car_rec.run_test()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z7nMg1K7AG-u","colab_type":"text"},"source":["## Extras\n","In case you want to know more and try some other, more cool, games, we invite you to test the NEAT algorithm and the class provided above with, for example, the [Space Invaders](https://gym.openai.com/envs/SpaceInvaders-ram-v0/) Gym environment. You can also follow [Lucas Thompson's](https://www.youtube.com/playlist?list=PLTWFMbPFsvz3CeozHfeuJIXWAJMkPtAdS) tutorial on using NEAT to play Sonic. Here is a link to his code: https://gitlab.com/lucasrthompson/Sonic-Bot-In-OpenAI-and-NEAT/tree/master. Keep in mind these type of games require much more time to find a solution, in the order of several hours, even days.\n","Here is another [attemp](https://medium.freecodecamp.org/how-to-use-ai-to-play-sonic-the-hedgehog-its-neat-9d862a2aef98) in running sonic."]},{"cell_type":"markdown","metadata":{"id":"TZAiHiDiBO0E","colab_type":"text"},"source":["Thank you for your time! Hope you have had an amazing 3 days with us!"]},{"cell_type":"code","metadata":{"id":"jKX9wCQ8BYQ2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}